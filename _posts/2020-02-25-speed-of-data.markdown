---
layout: post
title:  "Speed of Data"
date:   2018-05-04 06:19:57 -0600
categories: discussion
---
**Speed of Data**

Robert Koch, Enterprise Architect

28 March 2020

[!!unedited!!]

I've been thinking about this topic for quite some time and while I'm putting this into words, I'm hopeful to make this part of a larger discussion.  I wanted to talk about the speed of the data as it traverse through your business systems. As you know data sitting there has no value. Quality checks slows down the data thus lessening it's value for the sake of urgency. Some places who might be spitting out data in the raw nilly-willy ![sugar in the raw]({{ "/assets/sugar-in-the-raw.png" | absolute_url }}) would perhaps have "unverified" stamps on their data so their customers know that the data is received but not validated yet. Much of the data I work with in my career have data that are verified after a review period. The review process will take time since there could be human elements involved while verifying data. If you think about it, even if the data validation is automated, don't forget that the process is designed by humans, programmed by humans, tested by humans, and used by humans. Can never not be touched by humans! Not every scenario can be accounted for at the time of testing so there'll be something else that didn't work when validating the data, but we might not know what yet. Still involves that pesky human element.

Most of the data end users see (or prefer to see) are data that is usually aggregated and visualized. Aggregations over raw data helps us see trends right away and can detect errors out of the bat. For example, new data coming in may be way off compared to yesterday's data but you cannot see it when comparing against huge volumes of raw data (too much static). When we aggregate as groups, we can see anomalies better. Another way to verify data is to use visualizations -- charts -- when a visualization shows unexpected swings, this is an good indicator to verify the data at the source. Again, with end users verifing the data, no matter how much we try to automate validations from start to end, we need to build in intinution -- the capability of having a "bad feeling" ![Han Solo saying bad feeling]({{ "/assets/bad-feeling.png" | absolute_url }}). Even with a great AI that helps drive the car for you, although tested up the wazoo, we will still have minor flaws that causes accidents or scenarios that we didn't account for. Reagrdless, we (humans!) are constantly learning from it and making the data better and more intelligent for the consuming systems.

How can we attempt in automating validation? I'm of the opinion that we need to create an completely independent and separate system to digest the data and make sure it makes sense within human-defined boundaries since AI won't be much help at this point. APIs exposed to the datasets are an ideal approach where we may tap into the data source and independently verify it. Our systems are tainted or opininated with a scrum team that programs in, say, C# and then the automated process is done using the system of their choice. We should be crafting validation intelligence with a completely different system, maybe something with Python with Pandas/Dask or perhaps use Spark that forces us to view numbers in a different way. From there we can identify potential flaws in the datasets being analyzed. Years ago before I got involved with software development, I would verify the data being produced using CheckFree's Recon-Plus software to compare and identify differences. The data was produced by a Java-based system and using something different enables us to be unbiased about what we're looking for. 

Since whomever is always verifying the data with predefined quality control in place, the speed of the data will slow to a crawl that may not placate the customers, internal or external. Some days there may be a bad data discrepancy putting a halt on the whole validation process, thus embargoed. Now we have a pause in the workflow which decreases the value of the data with every passing minute. SOmeone has to go in and clean up the data and make the process flow again. This is an on-going challenge for any shop that puts much value on the timeliness of the data.

How may we get the speed of data to increase in light of all those barriers, be it quality, validation, or embargoes in place? One way is to convert the system into a more event-driven system where you'd have data coming in real-time and conduct aggregations on the fly. When we break down data into minuate and analyze it piece by piece as they come in, we can do quicker quality checks using algothrim-driven evaluations to ensure the validity. Compare that to a batch-driven system where huge volume of records has to be analyzed as a whole and if you tried to do it piecemeal, performance can be degraded to the point where it may not be worthwhile. So again, using an enterprise message bus, we can analyze data as it comes in and if there's a new scenario that we haven't thought of but want to implement and check for, we can just plug in the new algorithim in the message bus pipeline without adversely affecting performance or causing bottlenecks. This is a great example of how event-driven systems makes it easier for disparate development teams to come together. Granted, setting up an event-driven system is a huge undertaking and frought with mistakes and stalls due to learning curves, organizational dynamics, and prioritzation, however once it gets going, you're pretty much set to leverage the advantages of being event-driven.

Event-driven systems adds speed and value to the incoming data as it moves within your workflow. The data is always moving and being streamed somewhere. Imagine for a minute, you're watching the World Series on  television, and there's an intentional tape delay of 30 minutes -- it's not too bad, right? However, will the viewer appreciate it or they'll turn off the TV and would rather watch web-based streamed statistics as it comes online? I would say as a baseball fan myself, this would turn me off as a viewer and instead I might watch something else or do something else while glancing at real-time statistics. I'm hoping this analogy helps you understand where I'm going keeping the data flowing and moving to the end user, AKA those who spend the money.

Data is like economics, the study of money. Let's say you deposit $1,000 in the bank, the banker would loan out $800 to someone else, thus creating $1,800 circulating in the economy. Same way with your data. You have ingested data into your system, that an value of X, then system spits out added value to customers, that's Y, so now we have X + Y value in your data stores. Data, as it traverses thorough your system, being augmented and shared -- the more it get attached to other processes, the value of the said data can be tripled or quadrupled at that point.

Data is often the "truth". When you engage in truthful endeavours, everyone benfits! The ingestor and the consumer adds value to the data also. We always have immutable and maleable phases of the data, these hopefully gets recorded and auditable. Making the data auditable while maintain the speed it traverses in your organization is important -- for example, using blockchain is a great example of data that should not be immutable and recorded in sequential form. Regulators loves the technology as it makes the data more truthful and there's a trusted audit trail. Currently today, "trusted" and "audit" is a statisitical probablility exercise, why not make it "truthful"?

Cloud technologies makes it easier to do the above since we won't have to wait too long to provision services and with data traveling at the speed of light (or as close to it). We can orchestrate the best available tool for the job quickly through the cloud since servers do not have any inherit limitations, e.g. disk space, network bandwidth, proximity between services, and shared security. Tooling works well provided it's available through a common API or using transititional data points such as CSV files or through the event bus. CSVs will slow the data velocity but it is a ubiquitous data format and many software tools has the capability to ingest it. 

Having an event messaging bus is a great approach in keeping the data moving and adding value, however the tooling ingesting events are more limited compared to say ingesting CSV files. Today, investing in those tooling to read events and to extract more value is arguably more than what you would have invest (benefit + reward > cost). Adding event bus capabilities is usually a one-time hard work, then the rest is gravy. I have provided an example of the benefits of switching over from batch-based processing to event-driven processing in my ![prior blog post]().

Being event-based is not only limited to having a messaging bus, instead we can use that term loosely. Containers is a good example of an event-driven system where we can fire up containers to isolate workflows to minimize impact (or blast points). When a process needs to be run and there's memory constraints, containers can be the answer to this, either by spinning up another container to run the process or increasing memory on the existing containers (probably requires a restart). Having containers can be an important aspect in having an event-based solution that helps get your data to where it needs to be as quickest as possible and in a scalable way! Containers are great in helping increase data velocity and thus it's value.

Going serverless (the cloud) is another way to get increasing data velocity. Because serverless components offers more resiliency and it is scalable out of the box, using the cloud framework is a good way to implement. The tooling from cloud providers are very mature nowadays and with very little effort, you can get a system up and running in no time complete with ingestion and traversing through an event-driven pipeline, then generating artifacts and visualizations.

I hope from reading this you can get some ideas on how you may increase the speed of data in your workflow while adding value to it. Your goal is to attain enlightment with your data, it's a hard process frought with roadblocks, but the journey there is worth it. So there's work to be done, get to it!